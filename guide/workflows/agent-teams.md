# Agent Teams Workflow

> **Multi-agent parallel coordination for complex tasks**
> **Status**: Experimental (v2.1.32+) | **Model**: Opus 4.6+ required | **Flag**: `CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=1`

**What**: Multiple Claude instances work in parallel on a shared codebase, coordinating autonomously without active human intervention. One session acts as team lead to break down tasks and synthesize findings from teammates.

**When introduced**: v2.1.32 (2026-02-05) as research preview
**Reading time**: ~30 min
**Prerequisites**: Opus 4.6 model, understanding of [Sub-Agents](../ultimate-guide.md#sub-agents), familiarity with [Task Tool](../ultimate-guide.md#task-tool)

**üöÄ Want to get started fast?** See **[Agent Teams Quick Start Guide](./agent-teams-quick-start.md)** (8-10 min, copy-paste patterns for your projects)

---

## Table of Contents

1. [Overview](#1-overview)
2. [Architecture Deep-Dive](#2-architecture-deep-dive)
3. [Setup & Configuration](#3-setup--configuration)
4. [Production Use Cases](#4-production-use-cases)
5. [Workflow Impact Analysis](#5-workflow-impact-analysis)
6. [Limitations & Gotchas](#6-limitations--gotchas)
7. [Decision Framework](#7-decision-framework)
8. [Best Practices](#8-best-practices)
9. [Troubleshooting](#9-troubleshooting)
10. [Sources](#10-sources)

---

## 1. Overview

### What Are Agent Teams?

Agent teams enable **multiple Claude instances to work in parallel** on different subtasks while coordinating through a git-based system. Unlike manual multi-instance workflows where you orchestrate separate Claude sessions yourself, agent teams provide built-in coordination where agents claim tasks, merge changes continuously, and resolve conflicts automatically.

**Key characteristics**:
- ‚úÖ **Autonomous coordination** ‚Äî Team lead delegates, teammates communicate via mailbox
- ‚úÖ **Peer-to-peer messaging** ‚Äî Direct communication between agents (not just hierarchical)
- ‚úÖ **Git-based locking** ‚Äî Agents claim tasks by writing to shared directory
- ‚úÖ **Continuous merge** ‚Äî Changes pulled/pushed without manual intervention
- ‚úÖ **Independent context** ‚Äî Each agent has own 1M token context window (isolated)
- ‚ö†Ô∏è **Experimental** ‚Äî Research preview, stability not guaranteed
- ‚ö†Ô∏è **Token-intensive** ‚Äî Multiple simultaneous model calls = high cost

### When Introduced

**Version**: v2.1.32 (2026-02-05)
**Model**: Opus 4.6 minimum
**Status**: Research preview (experimental feature flag required)

**Official announcement**:
> "We've introduced agent teams in Claude Code as a research preview. You can now spin up multiple agents that work in parallel as a team and coordinate autonomously on shared codebases."
> ‚Äî [Anthropic, Introducing Claude Opus 4.6](https://www.anthropic.com/news/claude-opus-4-6)

> **üìù Documentation Update (2026-02-09)**: Architecture section corrected based on [Addy Osmani's research](https://addyosmani.com/blog/claude-code-agent-teams/). Key clarification: Agents communicate via **peer-to-peer messaging** through a mailbox system, not only through team lead synthesis. Context windows remain isolated (1M tokens per agent), but explicit messaging enables direct coordination between teammates.

### Agent Teams vs Other Patterns

| Pattern | Coordination | Setup | Best For |
|---------|--------------|-------|----------|
| **Agent Teams** | Automatic (built-in) | Experimental flag | Complex read-heavy tasks requiring coordination |
| **Multi-Instance** | Manual (human orchestration) | Multiple terminals | Independent parallel tasks, no coordination needed |
| **Dual-Instance** | Manual (human oversight) | 2 terminals | Quality assurance, plan-execute separation |
| **Task Tool** | Automatic (sub-agents) | Native feature | Single-agent task delegation, sequential work |

**Key distinction**:
- **Multi-Instance** = You manage coordination (separate projects, no shared state)
- **Agent Teams** = Claude manages coordination (shared codebase, git-based communication)

---

## 2. Architecture Deep-Dive

### Lead-Teammate Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Team Lead (Main Session)                ‚îÇ
‚îÇ  - Breaks tasks into subtasks                   ‚îÇ
‚îÇ  - Spawns teammate sessions                     ‚îÇ
‚îÇ  - Synthesizes findings from all agents         ‚îÇ
‚îÇ  - Coordinates via shared task list + mailbox   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                   ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Teammate 1    ‚îÇ‚óÑ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  Teammate 2    ‚îÇ
‚îÇ                ‚îÇ  ‚îÇ Peer-to-peer    ‚îÇ                ‚îÇ
‚îÇ - Own context  ‚îÇ  ‚îÇ messaging via   ‚îÇ - Own context  ‚îÇ
‚îÇ   (1M tokens)  ‚îÇ  ‚îÇ mailbox system  ‚îÇ   (1M tokens)  ‚îÇ
‚îÇ - Claims tasks ‚îÇ  ‚îÇ                 ‚îÇ - Claims tasks ‚îÇ
‚îÇ - Messages     ‚îÇ  ‚îÇ                 ‚îÇ - Messages     ‚îÇ
‚îÇ   team/peers   ‚îÇ  ‚îÇ                 ‚îÇ   team/peers   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Git-Based Coordination

**How it works**:

1. **Task claiming**: Agents write lock files to shared directory (`.claude/tasks/`)
2. **Work execution**: Each agent works independently in its context
3. **Continuous merge**: Agents pull/push changes to shared git repository
4. **Conflict resolution**: Automatic merge (with limitations, see [¬ß6](#6-limitations--gotchas))
5. **Result synthesis**: Team lead collects findings and presents unified response

**Example lock file structure**:
```
.claude/tasks/
‚îú‚îÄ‚îÄ task-1.lock        # Agent A claimed
‚îú‚îÄ‚îÄ task-2.lock        # Agent B claimed
‚îî‚îÄ‚îÄ task-3.pending     # Not yet claimed
```

### Communication Architecture

**Key distinction from sub-agents**: Agent teams implement **true peer-to-peer messaging** via a mailbox system, not just hierarchical reporting.

**Architecture components** (Source: [Addy Osmani](https://addyosmani.com/blog/claude-code-agent-teams/), Feb 2026):

1. **Team lead**: Creates team, spawns teammates, coordinates work
2. **Teammates**: Independent Claude Code instances with own context (1M tokens each)
3. **Task list**: Shared work items with dependency tracking and auto-unblocking
4. **Mailbox**: Inbox-based messaging system enabling direct communication between agents

**Communication patterns**:
- **Lead ‚Üí Teammate**: Direct messages or broadcasts to all
- **Teammate ‚Üí Lead**: Progress updates, questions, findings
- **Teammate ‚Üî Teammate**: Direct peer-to-peer messaging (challenge approaches, debate solutions)
- **Final synthesis**: Team lead aggregates all findings for user

**Example messaging flow**:
```
Team Lead: "Review this PR for security issues"
‚îú‚îÄ Teammate 1 (Security): Analyzes ‚Üí Messages Teammate 2: "Found auth issue in line 45"
‚îú‚îÄ Teammate 2 (Code Quality): Reviews ‚Üí Messages back: "Confirmed, also see OWASP violation"
‚îî‚îÄ Team Lead: Synthesizes findings ‚Üí Presents unified response to user
```

**What this enables**:
- ‚úÖ Agents actively challenge each other's approaches
- ‚úÖ Debate solutions without human intervention
- ‚úÖ Coordinate independently (self-organization)
- ‚úÖ Share discoveries mid-workflow (via messages, not context)

**Limitation**: Context isolation remains‚Äîagents don't share their full context window, only explicit messages.

### Navigation Between Agents

**Built-in navigation**:
- **Shift+Up/Down**: Switch between sub-agents in Claude Code interface
- **tmux**: Use tmux commands if running in tmux session
- **Direct takeover**: You can take control of any agent's work when needed

**Example**:
```bash
# Terminal 1: Team lead
claude --experimental-agent-teams

# Claude spawns teammates automatically
# You can navigate with Shift+Up/Down to inspect each agent
```

### Context Management

**Per-agent context**:
- Each agent has **1M token context window** (Opus 4.6)
- ~30,000 lines of code per session
- **Context isolation**: Agents don't share their full context window
- **Communication**: Via mailbox system (peer-to-peer + team lead synthesis)

**Total context capacity** (3 agents example):
- Team lead: 1M tokens
- Teammate 1: 1M tokens
- Teammate 2: 1M tokens
- **Total**: 3M tokens across team (context isolated, but communicating via messages)

**Important distinction**:
- ‚ùå **Context NOT shared**: Agent 1's full 1M token context invisible to Agent 2
- ‚úÖ **Messages ARE shared**: Agents send explicit messages via mailbox (findings, questions, debates)

---

## 3. Setup & Configuration

### Prerequisites

**Required**:
- ‚úÖ Claude Code v2.1.32 or later
- ‚úÖ Opus 4.6 model (`/model opus`)
- ‚úÖ Git repository (for coordination)

**Recommended**:
- ‚úÖ Understanding of [Sub-Agents](../ultimate-guide.md#sub-agents)
- ‚úÖ Familiarity with git workflows
- ‚úÖ Budget awareness (token-intensive feature)

### Method 1: Environment Variable

**Simplest approach** ‚Äî Set env var before starting Claude Code:

```bash
# Enable agent teams for this session
export CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=1

# Start Claude Code
claude
```

**Persistent setup** (bash/zsh):
```bash
# Add to ~/.bashrc or ~/.zshrc
echo 'export CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=1' >> ~/.bashrc
source ~/.bashrc
```

### Method 2: Settings File

**Persistent configuration** ‚Äî Edit `~/.claude/settings.json`:

```json
{
  "experimental": {
    "agentTeams": true
  }
}
```

**Advantages**:
- ‚úÖ Persistent across sessions
- ‚úÖ No need to remember env var
- ‚úÖ Can be version-controlled in dotfiles

**After editing**, restart Claude Code for changes to take effect.

### Verification

**Check if enabled**:

```bash
# In Claude Code session
> Are agent teams enabled?
```

Claude should confirm:
> "Yes, agent teams are enabled (experimental feature). I can spawn multiple agents to work in parallel when appropriate."

**Alternative verification** (check settings):
```bash
cat ~/.claude/settings.json | grep agentTeams
```

### Multi-Terminal Setup

**Pattern** (from practitioner reports):

```bash
# Terminal 1: Research + bugfix
export CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=1
claude --session research-bugfix

# Terminal 2: Business ops
export CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=1
claude --session business-ops

# Terminal 3: Infrastructure
export CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=1
claude --session infra-setup
```

**Benefits**:
- Isolation of contexts (research vs execution vs setup)
- Parallel progress on independent workstreams
- Reduced context switching cognitive load

**Note**: This is different from automatic teammate spawning ‚Äî here you're manually creating multiple team lead sessions. Each can spawn its own teammates.

---

## 4. Production Use Cases

### Overview of Validated Cases

| Use Case | Source | Metrics | Best For |
|----------|--------|---------|----------|
| **Multi-layer code review** | Fountain (Anthropic Report) | 50% faster screening | Security + API + Frontend simultaneous review |
| **Full dev lifecycle** | CRED (Anthropic Report) | 2x execution speed | 15M users, financial services compliance |
| **Autonomous C compiler** | Anthropic Research | Project completion | Complex multi-phase projects |
| **Job search app** | Paul Rayner (LinkedIn) | "Pretty impressive" | Design research + bug fixing |
| **Business ops automation** | Paul Rayner (LinkedIn) | N/A | Operating system + conference planning |

### 4.1 Multi-Layer Code Review (Fountain)

**Organization**: Fountain (frontline workforce management platform)
**Challenge**: Comprehensive codebase review across multiple concerns (security, API design, frontend)
**Solution**: Deployed hierarchical multi-agent orchestration with scope-focused sub-agents

**Agent scopes** (Fountain's approach):
- **Scope 1 (Security)**: Scan for vulnerabilities, auth issues, data exposure
- **Scope 2 (API)**: Review endpoint design, request/response validation, error handling
- **Scope 3 (Frontend)**: Check UI patterns, accessibility, performance

**Results**:
- ‚úÖ **50% faster** candidate screening
- ‚úÖ **40% quicker** onboarding
- ‚úÖ **2x candidate conversions**

**Why it worked**:
- **Read-heavy task**: Code review = primarily reading/analyzing (no write conflicts)
- **Clear domain separation**: Security, API, Frontend have minimal overlap
- **Independent analysis**: Each agent can work without waiting for others

**Example prompt** (team lead):
```
Review this PR comprehensively with scope-focused analysis:
- Security Scope: Check for vulnerabilities and auth issues (context: auth code, input validation)
- API Design Scope: Review endpoint design and error handling (context: API routes, controllers)
- Frontend Scope: Check UI patterns and accessibility (context: components, styles)

PR: https://github.com/company/repo/pull/123
```

**Source**: [2026 Agentic Coding Trends Report](https://resources.anthropic.com/hubfs/2026%20Agentic%20Coding%20Trends%20Report.pdf), Anthropic, Jan 2026

### 4.2 Full Development Lifecycle (CRED)

**Organization**: CRED (15M+ users, financial services, India)
**Challenge**: Accelerate delivery while maintaining quality standards essential for financial services
**Solution**: Implemented Claude Code across entire development lifecycle with agent teams for complex tasks

**Results**:
- ‚úÖ **2x execution speed** across development lifecycle
- ‚úÖ Maintained compliance (financial services standards)
- ‚úÖ Quality assurance preserved

**Why it worked**:
- **Large codebase**: 15M users = complex system requiring parallel analysis
- **Quality critical**: Financial services = need multiple validation layers
- **Tight deadlines**: Speed requirement justified token cost

**Workflow pattern**:
1. **Planning phase**: Team lead breaks down feature
2. **Implementation**: Teammate 1 = backend, Teammate 2 = frontend, Teammate 3 = tests
3. **Quality assurance**: Team lead synthesizes + runs validation
4. **Compliance check**: Final review against financial standards

**Source**: [2026 Agentic Coding Trends Report](https://resources.anthropic.com/hubfs/2026%20Agentic%20Coding%20Trends%20Report.pdf), Anthropic, Jan 2026

### 4.3 Autonomous C Compiler (Anthropic Research)

**Project**: Build an entire C compiler autonomously
**Challenge**: Multi-phase project (lexer, parser, AST, code generation, optimization) requiring coordination
**Solution**: Agent teams with task decomposition and progress tracking

**Phases completed**:
1. **Lexer**: Tokenization logic
2. **Parser**: Syntax tree construction
3. **AST**: Abstract syntax tree implementation
4. **Code generation**: Assembly output
5. **Optimization**: Performance improvements
6. **Testing**: Compiler test suite

**Results**:
- ‚úÖ **Project completed** without human intervention
- ‚úÖ All phases coordinated successfully
- ‚úÖ Tests passing at completion

**Why it worked**:
- **Clear phases**: Each compiler phase is well-defined (lexer ‚Üí parser ‚Üí codegen)
- **Minimal dependencies**: Phases have clear interfaces (tokens ‚Üí AST ‚Üí assembly)
- **Testable milestones**: Each phase verifiable independently

**Architecture insight**:
> "Individual agents break the project into small pieces, track progress, and determine next steps until completion."
> ‚Äî [Building a C compiler with agent teams](https://www.anthropic.com/engineering/building-c-compiler), Anthropic Engineering, Feb 2026

**Key learnings**:
- ‚ö†Ô∏è **Tests passing ‚â† correctness**: Human oversight still important for quality assurance
- ‚ö†Ô∏è **Verification required**: Automated success doesn't guarantee error-free code
- ‚úÖ **Feasibility proven**: Complex multi-phase projects achievable with agent teams

**Source**: [Building a C compiler with agent teams](https://www.anthropic.com/engineering/building-c-compiler), Anthropic Engineering, Feb 2026

### 4.4 Job Search App Development (Paul Rayner)

**Practitioner**: Paul Rayner (CEO Virtual Genius, EventStorming Handbook author, Explore DDD founder)
**Setup**: 3 concurrent agent team sessions across separate terminals
**Date**: Feb 2026 (v2.1.32 release day)

**Workflow 1 - Job Search App**:
- **Context**: Custom job search application development
- **Tasks**:
  - Design options research (explore UI/UX patterns)
  - Bug fixing in existing codebase
- **Pattern**: Research + execution in same workflow

**Workflow 2 - Business Operations**:
- **Context**: Operating system development + conference planning
- **Tasks**:
  - Business operating system automation
  - Conference planning resources (Explore DDD)
- **Pattern**: Multi-domain business tooling

**Workflow 3 - Infrastructure + Framework**:
- **Context**: Testing infrastructure + framework integration
- **Tasks**:
  - Playwright MCP instances setup
  - Beads framework management (Steve Yegge)
- **Pattern**: Infrastructure + framework coordination

**Results**:
- ‚úÖ "Pretty impressive" (subjective, no metrics)
- ‚úÖ Better than previous multi-terminal workflows without coordination
- ‚úÖ 3 independent contexts running simultaneously

**Why notable**:
- **Real-world validation**: Production usage by experienced practitioner
- **Multi-context**: 3 different domains (product, business, infra) simultaneously
- **Early adoption**: Posted same day as v2.1.32 release (early adopter signal)

**Open question raised**:
> "I'm not sure about Claude's guidance on when to use beads versus agent team sessions. Any thoughts?"
> ‚Äî Paul Rayner, LinkedIn, Feb 2026

**Source**: [Paul Rayner LinkedIn](https://www.linkedin.com/posts/thepaulrayner_this-is-wild-i-just-upgraded-claude-code-activity-7425635159678414850-MNyv), Feb 2026

### 4.5 Parallel Hypothesis Testing (Pattern)

**Scenario**: Debugging a complex production issue with multiple potential root causes

**Setup**:
```
Team lead prompt:
"Production API is slow. Test these hypotheses in parallel:
- Hypothesis 1 (DB): Query performance issue
- Hypothesis 2 (Network): Latency spikes
- Hypothesis 3 (Cache): Invalidation problem
Each agent: profile, reproduce, report findings"
```

**Agent assignments**:
- **Agent 1**: Database profiling (slow query log, explain plans)
- **Agent 2**: Network analysis (latency metrics, trace routes)
- **Agent 3**: Cache behavior (hit rates, invalidation patterns)

**Benefits**:
- ‚úÖ **Parallel investigation**: 3 hypotheses tested simultaneously (vs sequential)
- ‚úÖ **Time savings**: 1/3 of sequential debugging time
- ‚úÖ **Comprehensive**: No hypothesis ignored due to time constraints

**When to use**:
- Multiple plausible explanations for observed behavior
- Each hypothesis testable independently
- Time-critical debugging (production issues)

### 4.6 Large-Scale Refactoring (Pattern)

**Scenario**: Refactor authentication system across 47 files (frontend + backend + tests)

**Setup**:
```
Team lead prompt:
"Refactor auth system from JWT to OAuth2:
- Agent 1: Backend endpoints (/api/auth/*)
- Agent 2: Frontend components (src/components/auth/*)
- Agent 3: Integration tests (tests/auth/)
Coordinate changes via shared interfaces"
```

**Agent assignments**:
- **Agent 1**: Backend implementation (15 files)
- **Agent 2**: Frontend UI update (20 files)
- **Agent 3**: Test suite update (12 files)

**Benefits**:
- ‚úÖ **Context preservation**: All 47 files in one coordinated session (vs losing context after ~15)
- ‚úÖ **Interface consistency**: Shared contracts enforced across agents
- ‚úÖ **Atomic migration**: All layers updated in coordination

**Gotcha**:
- ‚ö†Ô∏è **Merge conflicts**: If agents modify same files (e.g., shared types)
- ‚ö†Ô∏è **Mitigation**: Clear interface boundaries, minimize shared file modifications

---

## 5. Workflow Impact Analysis

### Before/After Comparison

**Context**: What changes when using agent teams vs single-agent sessions?

| Task | Single Agent (Before) | Agent Teams (After) |
|------|-----------------------|---------------------|
| **Bug tracing** | Feed files one by one, re-explain architecture each time | See entire codebase at once, trace full data flow across all layers |
| **Code review** | Manually summarize PR yourself, explain context in prompt | Feed entire diff + surrounding code, agents read directly |
| **New feature** | Describe codebase structure in prompt (limited by your understanding) | Let agents read codebase directly, discover patterns themselves |
| **Refactoring** | Lose context after ~15 files, split into multiple sessions | All 47+ files live in one coordinated session |
| **Multi-service debugging** | Debug one service at a time, manually track cross-service flows | Parallel investigation across all involved services |

**Source**: [Claude Opus 4.6 for Developers](https://dev.to/thegdsks/claude-opus-46-for-developers-agent-teams-1m-context-and-what-actually-matters-4h8c), dev.to, Feb 2026

### Context Management Improvements

**Single agent limitations**:
- ~15 files before context management becomes challenging
- Manual summarization required for large codebases
- Sequential analysis of independent components

**Agent teams capabilities**:
- **1M tokens per agent** = ~30,000 lines of code
- **3 agents** = effectively 90,000 lines across team (isolated contexts)
- **Parallel reading**: Agents consume codebase sections simultaneously
- **Synthesis**: Team lead combines findings without context loss

**Example**:
```
Scenario: Analyze 28,000-line TypeScript service

Single agent:
- Read files sequentially
- Context pressure at ~15 files
- Manual summarization
- ~2-3 hours

Agent teams:
- Agent 1: Controllers layer (10K lines)
- Agent 2: Services layer (10K lines)
- Agent 3: Data layer (8K lines)
- Team lead: Synthesize architecture
- ~45 minutes
```

### Coordination Benefits

**Built-in vs manual coordination**:

| Aspect | Manual Multi-Instance | Agent Teams |
|--------|----------------------|-------------|
| **Task delegation** | You decide splits | Team lead decides |
| **Progress tracking** | Manual check-ins | Automatic reporting |
| **Merge conflicts** | You resolve | Automatic (with limitations) |
| **Context sharing** | Copy-paste findings | Git-based coordination |
| **Cognitive load** | High (orchestrator role) | Low (observer role) |

**When coordination matters**:
- ‚úÖ Tasks with dependencies (Feature A needs API from Feature B)
- ‚úÖ Shared interfaces (multiple agents modify same contract)
- ‚úÖ Quality gates (all agents must pass before merge)

**When coordination unnecessary**:
- ‚ùå Completely independent tasks (separate projects)
- ‚ùå No shared state (different repositories)
- ‚ùå Simple parallelization (run same script on different data)

### Cost Trade-offs

**Token consumption comparison** (estimated):

| Workflow | Single Agent | Agent Teams (3) | Multiplier |
|----------|-------------|-----------------|------------|
| **Code review (small PR)** | 10K tokens | 25K tokens | 2.5x |
| **Code review (large PR)** | 50K tokens | 90K tokens | 1.8x |
| **Bug investigation** | 30K tokens | 70K tokens | 2.3x |
| **Feature implementation** | 100K tokens | 200K tokens | 2x |
| **Refactoring (large)** | 150K tokens | 250K tokens | 1.7x |

**Cost justification scenarios**:
- ‚úÖ **Time-critical**: Production issues requiring fast resolution
- ‚úÖ **Complexity**: Multi-layer analysis (security + performance + architecture)
- ‚úÖ **Quality**: High-stakes changes requiring multiple verification layers
- ‚ùå **Simple tasks**: Straightforward implementations (overkill)
- ‚ùå **Budget-constrained**: Personal projects with tight token limits

**Rule of thumb**: Agent teams justified when time saved > 2x token cost increase.

---

## 6. Limitations & Gotchas

### Read-Heavy vs Write-Heavy Trade-off

**Core limitation**: Agent teams excel at read-heavy tasks but struggle with write-heavy tasks where multiple agents modify the same files.

**Why this matters**:
```
Read-heavy (‚úÖ Good for teams):
- Code review: Agents read code, provide analysis
- Bug tracing: Agents read logs, trace execution
- Architecture analysis: Agents read structure, identify patterns

Write-heavy (‚ö†Ô∏è Risky for teams):
- Refactoring shared types: Multiple agents modify same file ‚Üí merge conflicts
- Database schema changes: Coordinated migrations across files
- API contract updates: Interface changes require synchronization
```

**Mitigation strategies**:
1. **Clear boundaries**: Assign non-overlapping file sets to agents
2. **Interface-first**: Define contracts before parallel implementation
3. **Single-writer pattern**: One agent writes shared files, others read only
4. **Human review**: Manually resolve merge conflicts when they occur

### Merge Conflict Scenarios

**Automatic resolution works**:
- ‚úÖ Different files modified by different agents
- ‚úÖ Different functions in same file (clean git merges)
- ‚úÖ Additive changes (new functions, no edits)

**Automatic resolution struggles**:
- ‚ùå Same lines modified (classic merge conflict)
- ‚ùå Conflicting logic (Agent A removes validation, Agent B adds it)
- ‚ùå Circular dependencies (Agent A needs Agent B's output, vice versa)

**Example conflict**:
```typescript
// Agent 1 changes:
function processUser(user: User) {
  validateEmail(user.email); // Added validation
  return save(user);
}

// Agent 2 changes (same time):
function processUser(user: User) {
  return save(sanitize(user)); // Added sanitization
}

// Conflict: Both modified same function
// Resolution: Human decides order (validate ‚Üí sanitize ‚Üí save)
```

### Token Intensity Implications

**Why token-intensive**:
- Each agent runs **separate model inference** (3 agents = 3x base cost)
- Context loading for each agent (1M tokens √ó 3 = 3M token capacity)
- Coordination overhead (team lead synthesis)

**Budget impact example** (Opus 4.6 pricing):
```
Single agent session:
- Input: 50K tokens @ $15/M = $0.75
- Output: 5K tokens @ $75/M = $0.38
- Total: $1.13

Agent teams (3 agents):
- Input: 150K tokens @ $15/M = $2.25
- Output: 15K tokens @ $75/M = $1.13
- Total: $3.38

Cost multiplier: 3x
```

**Justification required**:
- ‚úÖ Time saved > cost increase (production issues)
- ‚úÖ Quality critical (financial services, healthcare)
- ‚úÖ Complexity justifies parallelization (multi-layer analysis)
- ‚ùå Simple tasks (use single agent)
- ‚ùå Personal learning projects (budget-constrained)

### Experimental Status Caveats

**What "experimental" means**:
- ‚ö†Ô∏è **No stability guarantee**: Feature may change or be removed
- ‚ö†Ô∏è **Bugs expected**: Report issues to Anthropic (GitHub Issues)
- ‚ö†Ô∏è **Performance variability**: Coordination speed may fluctuate
- ‚ö†Ô∏è **Documentation evolving**: Official docs still minimal

**Production usage considerations**:
1. **Fallback plan**: Be ready to revert to single-agent if issues arise
2. **Monitoring**: Track token costs carefully (can escalate quickly)
3. **Validation**: Human review of agent team outputs (don't trust blindly)
4. **Feedback**: Report bugs/experiences to help Anthropic improve feature

**Practitioner reports** (as of Feb 2026):
- ‚úÖ Paul Rayner: "Pretty impressive" (production usage validated)
- ‚úÖ Fountain: 50% faster (deployed in production)
- ‚úÖ CRED: 2x speed (15M users, financial services)
- ‚ö†Ô∏è Community: Mixed reports (some merge conflict issues)

### Context Isolation

**What agents can't do**:
- ‚ùå **Share context windows**: Agent 1's full context (1M tokens) not visible to Agent 2
- ‚ùå **Auto-sync discoveries**: Agent 2 won't see Agent 1's findings unless explicitly messaged
- ‚ùå **Coordinate timing**: Agents work independently, may finish at different times

**What agents CAN do**:
- ‚úÖ **Send messages**: Via mailbox system (peer-to-peer or via team lead)
- ‚úÖ **Challenge approaches**: Debate solutions, ask questions to each other
- ‚úÖ **Share findings**: Explicit messaging (not automatic context sharing)

**Implications**:
```
Scenario: Agent 1 discovers critical bug that affects Agent 2's work

Without messaging:
- Agent 2 doesn't see Agent 1's discovery automatically
- Agent 2 may continue with flawed assumption

With messaging (built-in):
- Agent 1 messages Agent 2: "Found auth issue in line 45"
- Agent 2 adjusts approach based on message
- Team lead synthesizes all findings at end

Mitigation:
- Agents can message each other via mailbox system
- Team lead synthesizes findings after all agents complete
- Human can interrupt and redirect agents mid-workflow (Shift+Up/Down)
- Design tasks with minimal inter-agent dependencies
```

### When NOT to Use Agent Teams

**Single agent is better for**:
- ‚ùå **Simple tasks**: Straightforward implementations (overkill)
- ‚ùå **Small codebases**: <5 files affected (coordination overhead not justified)
- ‚ùå **Write-heavy tasks**: Lots of shared file modifications (merge conflict risk)
- ‚ùå **Sequential dependencies**: Task B requires Task A completion (no parallelization benefit)
- ‚ùå **Budget constraints**: Personal projects, learning (token cost multiplier)
- ‚ùå **Tight interdependencies**: Circular dependencies between tasks

**Example of poor fit**:
```
Task: Update authentication logic in shared auth.ts file

Why single agent better:
- One file modified (no parallelization benefit)
- Write-heavy (multiple changes to same file)
- No clear subtask boundaries (logic intertwined)
- Sequential flow (test after each change)

Result: Agent teams would create merge conflicts, no time savings
```

---

## 7. Decision Framework

### Teams vs Multi-Instance vs Dual-Instance

**Comparison table**:

| Criterion | Agent Teams | Multi-Instance | Dual-Instance |
|-----------|-------------|----------------|---------------|
| **Coordination** | Automatic (git-based + mailbox) | Manual (human) | Manual (human) |
| **Setup** | Experimental flag | Multiple terminals | 2 terminals |
| **Best for** | Read-heavy tasks needing coordination | Independent parallel tasks | Quality assurance (plan-execute split) |
| **Communication** | Peer-to-peer messaging + team lead synthesis | Manual copy-paste | Manual synchronization |
| **Context sharing** | Isolated (1M per agent, no auto-sync) | Isolated (separate sessions) | Isolated (2 sessions) |
| **Cost** | High (3x+ tokens) | Medium (2x tokens) | Medium (2x tokens) |
| **Cognitive load** | Low (observer) | High (orchestrator) | Medium (reviewer) |
| **Merge conflicts** | Automatic resolution (limited) | N/A (separate repos) | Manual resolution |
| **Maturity** | Experimental (v2.1.32+) | Stable | Stable |

### Decision Tree: When to Use Agent Teams

```
Start
  ‚îÇ
  ‚îú‚îÄ Task is simple (<5 files)? ‚îÄ‚îÄYES‚îÄ‚îÄ> Single agent
  ‚îÇ
  ‚îú‚îÄ NO
  ‚îÇ
  ‚îú‚îÄ Tasks completely independent? ‚îÄ‚îÄYES‚îÄ‚îÄ> Multi-Instance
  ‚îÇ
  ‚îú‚îÄ NO
  ‚îÇ
  ‚îú‚îÄ Need quality assurance split? ‚îÄ‚îÄYES‚îÄ‚îÄ> Dual-Instance
  ‚îÇ
  ‚îú‚îÄ NO
  ‚îÇ
  ‚îú‚îÄ Read-heavy (analysis, review)? ‚îÄ‚îÄYES‚îÄ‚îÄ> Agent Teams ‚úì
  ‚îÇ
  ‚îú‚îÄ NO
  ‚îÇ
  ‚îú‚îÄ Write-heavy (many file mods)? ‚îÄ‚îÄYES‚îÄ‚îÄ> Single agent
  ‚îÇ
  ‚îú‚îÄ NO
  ‚îÇ
  ‚îú‚îÄ Budget-constrained? ‚îÄ‚îÄYES‚îÄ‚îÄ> Single agent
  ‚îÇ
  ‚îú‚îÄ NO
  ‚îÇ
  ‚îî‚îÄ Complex coordination needed? ‚îÄ‚îÄYES‚îÄ‚îÄ> Agent Teams ‚úì
                                   ‚îÄ‚îÄNO‚îÄ‚îÄ> Single agent
```

### Use Case Mapping

**Agent Teams (‚úÖ Use)**:
- Multi-layer code review (security + API + frontend)
- Parallel hypothesis testing (debugging)
- Large-scale refactoring (clear boundaries)
- Full codebase analysis (architecture review)
- Complex feature research (explore multiple approaches)

**Multi-Instance (‚úÖ Use)**:
- Separate projects (frontend repo + backend repo)
- Independent features (no shared state)
- Different technologies (Python microservice + React app)
- Parallel experimentation (try 3 different architectures)

**Dual-Instance (‚úÖ Use)**:
- Plan-execute pattern (planning session + execution session)
- Quality review (implementation + code review)
- Test-first development (write tests + implement)

**Single Agent (‚úÖ Use)**:
- Simple implementations (<5 files)
- Write-heavy tasks (shared file modifications)
- Sequential workflows (step-by-step tutorials)
- Budget-constrained projects

### Teams vs Beads Framework

**Beads Framework** (Steve Yegge):
- **Architecture**: Event-sourced MCP server (Gas Town) + SQLite database (beads.db)
- **Coordination**: Persistent message storage, historical replay
- **Maturity**: Community-maintained, experimental
- **Setup**: Requires Gas Town installation + agent-chat UI
- **Use case**: On-prem/airgap environments, full control over orchestration

**Agent Teams** (Anthropic):
- **Architecture**: Native Claude Code feature, git-based coordination
- **Coordination**: Real-time git locking, automatic merge
- **Maturity**: Official Anthropic feature (experimental)
- **Setup**: Feature flag only (`CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=1`)
- **Use case**: Rapid prototyping, cloud-based development

**Comparison**:

| Aspect | Beads Framework | Agent Teams |
|--------|----------------|-------------|
| **Control** | Full (event sourcing, replay) | Limited (black-box coordination) |
| **Setup** | Complex (Gas Town + agent-chat) | Simple (feature flag) |
| **Persistence** | SQLite (beads.db) | Git commits |
| **Visibility** | agent-chat UI (Slack-like) | Native Claude Code interface |
| **Environment** | On-prem friendly | Cloud-first |
| **Maturity** | Community-driven | Anthropic official |

**When to use Beads**:
- ‚úÖ On-prem/airgap requirements (no cloud API calls)
- ‚úÖ Need event replay (debugging orchestration)
- ‚úÖ Custom orchestration logic (beyond git-based)
- ‚úÖ Persistent agent communications (audit trail)

**When to use Agent Teams**:
- ‚úÖ Cloud development (Anthropic API access)
- ‚úÖ Rapid setup (no infrastructure required)
- ‚úÖ Git-native workflows (already using git)
- ‚úÖ Official support path (Anthropic-maintained)

**Open question** (as of Feb 2026):
> "I'm not sure about Claude's guidance on when to use beads versus agent team sessions."
> ‚Äî Paul Rayner, Feb 2026

**Community feedback needed**: Anthropic has not published official guidance on this choice. Practitioners are invited to share experiences in [GitHub Discussions](https://github.com/anthropics/claude-code/discussions).

---

## 8. Best Practices

### Task Decomposition Strategies

**Clear boundaries principle**:
```
Good decomposition:
- Agent 1: Backend API endpoints (/api/users/*)
- Agent 2: Frontend components (src/components/users/*)
- Agent 3: Database migrations (db/migrations/users/)

Why good:
- Non-overlapping file sets (no merge conflicts)
- Clear interfaces (API contracts)
- Independent testing (each layer testable)
```

```
Bad decomposition:
- Agent 1: User authentication
- Agent 2: User authorization
- Agent 3: User session management

Why bad:
- Overlapping files (auth.ts touched by all 3)
- Interdependencies (auth needs sessions, sessions need auth)
- Sequential coupling (can't parallelize effectively)
```

**Interface-first approach**:
1. **Define contracts**: Agree on function signatures, API schemas before parallel work
2. **Type stubs**: Create TypeScript types/interfaces first, implement separately
3. **Mock boundaries**: Each agent works with mocked dependencies initially
4. **Integration phase**: Team lead coordinates final integration

**Example**:
```typescript
// Team lead defines interface first
interface UserService {
  authenticate(email: string, password: string): Promise<User>;
  authorize(user: User, resource: string): Promise<boolean>;
}

// Agent 1 implements authenticate
// Agent 2 implements authorize
// No merge conflicts (different functions)
```

### Coordination Patterns

**Fan-out, fan-in**:
```
Team lead
  ‚îÇ
  ‚îú‚îÄ Agent 1: Task A ‚îÄ‚îÄ‚îê
  ‚îú‚îÄ Agent 2: Task B ‚îÄ‚îÄ‚îº‚îÄ‚îÄ> Team lead synthesizes
  ‚îî‚îÄ Agent 3: Task C ‚îÄ‚îÄ‚îò
```

**Sequential phases with parallelization**:
```
Phase 1 (Sequential):
  Team lead: Define architecture

Phase 2 (Parallel):
  ‚îú‚îÄ Agent 1: Implement backend
  ‚îú‚îÄ Agent 2: Implement frontend
  ‚îî‚îÄ Agent 3: Write tests

Phase 3 (Sequential):
  Team lead: Integration + validation
```

**Hierarchical delegation**:
```
Team lead
  ‚îÇ
  ‚îú‚îÄ Agent 1 (Backend lead)
  ‚îÇ   ‚îú‚îÄ Agent 1a: Controllers
  ‚îÇ   ‚îî‚îÄ Agent 1b: Services
  ‚îÇ
  ‚îî‚îÄ Agent 2 (Frontend lead)
      ‚îú‚îÄ Agent 2a: Components
      ‚îî‚îÄ Agent 2b: State management
```

### Git Worktree Management

**Why worktrees matter**:
- Each agent works in separate git worktree (isolated file system)
- Prevents file locking conflicts
- Enables parallel file modifications

**Setup**:
```bash
# Main repository
git worktree add ../project-agent1 main

# Agent 1 works in project-agent1/
# Agent 2 works in project-agent2/
# Team lead works in project/

# All sync via git commits
```

**Best practices**:
- ‚úÖ One worktree per agent
- ‚úÖ Frequent commits (continuous merge)
- ‚úÖ Descriptive branch names (`agent1-backend-api`, `agent2-frontend-ui`)
- ‚ùå Don't modify same files across worktrees without coordination

### Cost Optimization

**Token-saving strategies**:

1. **Lazy spawning**: Only spawn agents when parallelization clearly benefits
   ```
   Bad: "Spawn 3 agents to implement this button"
   Good: "Spawn agents for multi-layer security review"
   ```

2. **Context pruning**: Remove irrelevant files from agent context
   ```
   # Tell agent what to ignore
   "Review backend API, ignore frontend files"
   ```

3. **Progressive escalation**: Start with single agent, escalate to teams if needed
   ```
   Step 1: Single agent attempts task
   Step 2: If complexity high, spawn team
   ```

4. **Result caching**: Reuse agent findings across similar tasks
   ```
   "Agent 1 found security issues in auth.ts.
   Agent 2, check if user.ts has same patterns."
   ```

### Quality Assurance

**Validation checklist**:
- [ ] **All agents completed**: No hanging tasks
- [ ] **Merge conflicts resolved**: Clean git history
- [ ] **Tests passing**: Automated test suite green
- [ ] **Human review**: Code inspection (don't trust blindly)
- [ ] **Cross-agent consistency**: Naming, patterns aligned

**Red flags**:
- ‚ö†Ô∏è Agents finished at very different times (imbalanced load)
- ‚ö†Ô∏è Many merge conflicts (poor task decomposition)
- ‚ö†Ô∏è Tests failing after merge (integration issues)
- ‚ö†Ô∏è Inconsistent code style (agents didn't follow shared standards)

**Mitigation**:
```bash
# After agent teams complete
git diff main..agent-teams-branch  # Review all changes
npm test                           # Run full test suite
npm run lint                       # Check code style
```

---

## 9. Troubleshooting

### Common Issues

#### Issue: Agents not spawning

**Symptoms**:
- Agent teams prompt accepted but no teammates created
- Only team lead session running

**Causes**:
1. Feature flag not set correctly
2. Model not Opus 4.6 (teams require Opus)
3. Task not complex enough (Claude decided single agent sufficient)

**Solutions**:
```bash
# Verify flag
echo $CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS  # Should output "1" or "true"

# Check settings
cat ~/.claude/settings.json | grep agentTeams  # Should be true

# Force model
/model opus

# Explicit request
"Spawn 3 agents for this task (team lead + 2 teammates)"
```

#### Issue: Merge conflicts overwhelming

**Symptoms**:
- Many git conflicts after agents complete
- Manual resolution required frequently

**Causes**:
- Poor task decomposition (overlapping file sets)
- Write-heavy task (multiple agents modifying shared files)

**Solutions**:
```
Prevention:
1. Clear boundaries: Non-overlapping file assignments
2. Interface-first: Define contracts before implementation
3. Single-writer: One agent writes shared files, others read

Recovery:
1. Revert: git reset --hard before-agent-teams
2. Sequential: Re-implement with single agent
3. Human merge: Manually resolve conflicts (git mergetool)
```

#### Issue: High token costs

**Symptoms**:
- Token usage 3x+ higher than expected
- Budget exhausted quickly

**Causes**:
- Over-spawning agents (3+ agents for simple tasks)
- Long-running sessions (agents idle)
- Large context per agent (1M tokens √ó 3)

**Solutions**:
```
Immediate:
1. Kill extra agents: Shift+Down, exit agent session
2. Reduce scope: Narrow task boundaries
3. Switch to single agent: /model sonnet (cheaper)

Long-term:
1. Cost monitoring: Track token usage per session
2. Lazy spawning: Only spawn when needed
3. Progressive escalation: Start small, scale up if needed
```

#### Issue: Agents stuck/hanging

**Symptoms**:
- One agent finishes, others still processing for long time
- No progress updates

**Causes**:
- Imbalanced task distribution (one agent has 80% of work)
- Agent waiting for dependency (sequential coupling)
- Bug in git coordination (rare)

**Solutions**:
```bash
# Navigate to stuck agent
Shift+Down  # Switch to agent

# Check status
"What are you working on? Progress update?"

# Manual takeover if needed
"Stop current task, report findings so far"

# Kill and redistribute
Exit agent ‚Üí Team lead redistributes task
```

#### Issue: Inconsistent results across agents

**Symptoms**:
- Agent 1 says "No issues", Agent 2 finds 10 bugs (same codebase)
- Conflicting recommendations

**Causes**:
- Different context windows (agents saw different files)
- Ambiguous instructions (agents interpreted differently)
- Model variability (stochastic outputs)

**Solutions**:
```
Prevention:
1. Explicit instructions: "All agents: Check for SQL injection"
2. Shared context: Point all agents to same reference docs
3. Validation: Human reviews all agent outputs

Recovery:
1. Reconciliation: "Compare Agent 1 and Agent 2 findings, resolve conflicts"
2. Third opinion: Spawn Agent 3 to arbitrate
3. Human decision: You choose which agent's recommendation to follow
```

### Navigation Problems

**Can't find agent sessions**:
```bash
# List all sessions
claude --list

# Filter for agent sessions
claude --list | grep agent

# Resume specific agent
claude --resume <session-id>
```

**Lost track of which agent is which**:
```
Solution: Name agents explicitly in team lead prompt

Good:
"Spawn 3 agents:
- Agent Security: Check vulnerabilities
- Agent Performance: Profile bottlenecks
- Agent Tests: Write test suite"

Bad:
"Spawn 3 agents for this codebase review"
```

**tmux navigation not working**:
```bash
# Verify tmux session
tmux list-sessions

# Attach to session
tmux attach -t claude-agents

# Navigate
Ctrl+b, n  # Next window
Ctrl+b, p  # Previous window
```

### Performance Optimization

**Slow coordination**:
```bash
# Check git repo size
du -sh .git/  # If >1GB, consider cleanup

# Clean up git objects
git gc --aggressive --prune=now

# Use shallow clone for agents
git clone --depth 1 <repo>
```

**Context loading delays**:
```
# Reduce context per agent
"Agent 1: Only load src/backend/* files"
"Agent 2: Only load src/frontend/* files"

# Prune irrelevant files
echo "node_modules/" >> .gitignore
echo "dist/" >> .gitignore
```

---

## 10. Sources

### Official Anthropic Sources

1. **[Introducing Claude Opus 4.6](https://www.anthropic.com/news/claude-opus-4-6)**
   Anthropic, Feb 2026
   Official announcement of Opus 4.6 and agent teams research preview

2. **[Building a C compiler with agent teams](https://www.anthropic.com/engineering/building-c-compiler)**
   Anthropic Engineering, Feb 2026
   Technical deep-dive: git-based coordination, autonomous C compiler case study

3. **[2026 Agentic Coding Trends Report](https://resources.anthropic.com/hubfs/2026%20Agentic%20Coding%20Trends%20Report.pdf)**
   Anthropic, Jan 2026
   Production metrics: Fountain (50% faster), CRED (2x speed)

### Community Sources

4. **[Claude Opus 4.6 for Developers: Agent Teams, 1M Context](https://dev.to/thegdsks/claude-opus-46-for-developers-agent-teams-1m-context-and-what-actually-matters-4h8c)**
   dev.to, Feb 2026
   Setup instructions, workflow impact table, read/write trade-offs

5. **[The best way to do agentic development in 2026](https://dev.to/chand1012/the-best-way-to-do-agentic-development-in-2026-14mn)**
   dev.to, Jan 2026
   Integration patterns: Claude Code + plugins (Conductor, Superpowers, Context7)

### Practitioner Testimonials

6. **[Paul Rayner LinkedIn Post](https://www.linkedin.com/posts/thepaulrayner_this-is-wild-i-just-upgraded-claude-code-activity-7425635159678414850-MNyv)**
   Paul Rayner (CEO Virtual Genius, EventStorming Handbook author), Feb 2026
   Production usage: 3 concurrent workflows (job search app, business ops, infrastructure)

### Related Documentation

- [Claude Code Releases](../claude-code-releases.md) ‚Äî v2.1.32, v2.1.33 release notes
- [Sub-Agents](../ultimate-guide.md#sub-agents) ‚Äî Single-agent task delegation
- [Multi-Instance Workflows](../ultimate-guide.md#multi-instance-workflows) ‚Äî Manual parallel coordination
- [Dual-Instance Pattern](../ultimate-guide.md#dual-instance-pattern) ‚Äî Plan-execute split
- [AI Ecosystem: Beads Framework](../ai-ecosystem.md#beads-framework) ‚Äî Alternative orchestration (Gas Town)

---

## Feedback & Contributions

**Experiencing issues?** Report to [Anthropic GitHub Issues](https://github.com/anthropics/claude-code/issues)

**Production learnings?** Share in [GitHub Discussions](https://github.com/anthropics/claude-code/discussions)

**Questions?** Ask in [Dev With AI Community](https://www.devw.ai/) (1500+ devs, Slack)

---

*Version 1.0.0 | Created: 2026-02-07 | Agent Teams (v2.1.32+, Experimental)*
